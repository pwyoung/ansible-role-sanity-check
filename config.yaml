---
################################################################################
# GLOBAL BOX DEFINITIONS
################################################################################
# There is some support for static IPs.
#   - On VirtualBox:
#     - 'ip', if given,  will be the IP on an internal network
#     - 'ip2', if given, will be the IP on an internal network
#     - 'netmask', if given, will be applies to the networks with 'ip' and/or 'ip2'
#   - On Libvirt, the values of 'ip', 'ip2', and 'netmask' (currently) don't currently matter.
#     - 'ip2', if given, cause a second NIC to be created (on 'libvir1')
#     - In future, we could automate management of QEMU networks (as we do with QEMU disks)
#   - Example:
#boxes:
#  - name: test-1
#    ip: 192.254.3.10
#    netmask: 255.255.255.128
#    ip2: 192.254.3.138
boxes:
  - name: test-1
  - name: test-2
  - name: test-3

################################################################################
# PROVISIONER: VAGRANT
################################################################################
vagrant_global:
  # Only create the first 'limit_num_boxes' boxes
  #limit_boxes: "yes"
  #limit_num_boxes: 1
  memory: 4096
  cpus: 4
  boot_timeout: 300
  # This ends up slightly different depending on the vagrant provider right now.
  # For Virtualbox: /dev/nvme0n1, /dev/nvme0n2, etc.
  # For libvirt:    /dev/nvme0n1, /dev/nvme1n1, etc.
  # This is due to how Virtualbox only supports one nvme storage controller
  nvme_disks:
    data:
      size: 102401
    data2:
      size: 102401
  #
  # You can use force_provider if your OS defaults differ from the default
  # heuristics on Vagrantfile.
  #force_provider: "libvirt"
  #force_provider: "virtualbox"
  #
  virtualbox_cfg:
    enabled: "true" # If not "true" then omit from Vagrantfile
    # https://app.vagrantup.com/ubuntu/boxes/bionic64
    box: "generic/ubuntu1804"
    enable_sse4: "true"
    # can be vdi, vmdk, vhd
    nvme_disk_postfix: 'vdi'
    # On the current directory
    nvme_disk_path: '.vagrant/nvme_disks/' # Default to a path relative to the Vagrantfile (common for VirtualBox)
    #nvme_disk_path: '/some-absolute-path/' # May be useful
    #nvme_disk_path: '$HOME/.local/share/libvirt/images/' # Some other path in the user's home directory
    nvme_disk_prefix: 'nvme_disk'
    # To stress test a virtual nvme controller you could peg all disks onto
    # one controller. We want to avoid this as our focus is testing filesystems
    # and not storage controllers however Virtualbox currently only supports
    # one nvme storage controller. Set this to true only if you are adding
    # support for this upstream to Virtualbox.
    nvme_controller_per_disk: false
    # If true, then append to 'nvme_disk_path' the name of the project.
    # The project name is taken from the parent directory of the directory holding the Vagrantfiile.
    # So, if the Vagrantfile is in /a/b/c/PROJECT_NAME/foo/Vagrantfile then 'PROJECT_NAME' is used
    append_project_name_to_nvme_disk_path: 'true'
  libvirt_cfg:
    enabled: "true" # If not "true" then omit from Vagrantfile
    # Setup with: vagrant box add <BOX> --provider=libvirt
    box: "generic/ubuntu1804"
    machine_type: 'q35'
    nvme_disk_postfix: 'qcow2'
    nvme_disk_id_prefix: 'drv'
    # Fedora uses qemu here by default. Override with the environment variable
    # KDEVOPS_VAGRANT_QEMU_GROUP. If not sure edit /etc/libvirt/qemu.conf user
    # and group settings. If using apparmor / selinux you may run into snags,
    # but that is out of scope of this project.
    qemu_group: 'qemu' # If this is uncommented, set the group on the storage files to this
    # In case you use a development version of qemu
    # emulator_path: '/usr/local/bin/qemu-system-x86_64'
    emulator_path: '/usr/bin/qemu-system-x86_64'
    # These are all valid
    nvme_disk_path: '$HOME/.local/share/libvirt/images/' # Default to something that works for QEMU://session
    #nvme_disk_path: '/var/lib/libvirt/images/' # Absolute path (for QEMU://sytem)
    #nvme_disk_path: '.vagrant/nvme_disks/' # Relative to the Vagrantfile (requires setting perms)
    #
    # If true, then append to 'nvme_disk_path' the name of the project.
    # The project name is taken from the parent directory of the directory holding the Vagrantfiile.
    # So, if the Vagrantfile is in /a/b/c/PROJECT_NAME/foo/Vagrantfile then 'PROJECT_NAME' is used
    append_project_name_to_nvme_disk_path: 'true'

################################################################################
# PROVISIONER: TERRAFORM (TODO)
################################################################################

################################################################################
# CONFIGURATION: VIA ANSIBLE
################################################################################
ansible:
  # build/requirements.yaml
  roles:
    - src: https://github.com/pwyoung/ansible-role-sanity-check
      name: ansible-role-sanity-check
      scm: git
      version: 0.0.2
  # build/playbooks/<playbooks> and build/run-playbooks.sh
  playbooks:
    #EXAMPLE
    #- name: "ansible-playbook-sanity-check"
    #  src: "https://github.com/pwyoung/ansible-playbook-sanity-check"
    #  method: git
    #  version: 0.0.1
    #  entry: playbook.yaml # Defaults to site.yml
    - name: ansible-playbook-deploy-kubespray
      scm: git
      # This is useful for doing development of this Ansible Playbook.
      # The config below assumes you are working in a branch called 'dev'.
      # If 'src' points to the local code, then this will check out the
      # latest 'dev' commit to ./build/playbooks/<name> and use it in the run.
      #src: file:///<PATH>/ansible-playbook-deploy-kubespray
      #version: dev      
      src: "https://github.com/pwyoung/ansible-playbook-deploy-kubespray.git"
      version: master
  # build/inventory.yml
  inventory:
    all:
      vars:
        ansible_python_interpreter: "/usr/bin/python3"
      hosts:
        test-1:
        test-2:
        test-3:
      children:
        kube-master:
          hosts:
            test-1:
            test-2:
        kube-test:
          hosts:
            test-1:
            test-2:
            test-3:
        etcd:
          hosts:
            test-1:
            test-2:
            test-3:
        k8s-cluster:
          children:
            kube-master:
            kube-test:
        calico-rr:
          hosts: {}
  # build/extra_vars.yml
  # NB: DON'T PUT SECRETS HERE
  # Special Embedded use here: 
  #   The playbook above, 'ansible-playbook-deploy-kubespray', installs Ansible onto the first master node
  #   and runs Kubespray there.
  #   That's done for portability, reproducability, and isolation (security) reasons.
  #   The local file (build/extra_vars.yaml) is copied to the control node and used to modulate the run there.
  #   So, make sure to not include any "real" secrets in this file (that can't be on the control node).
  extra_vars:
    # Kubespray generic/download parameters:
    #   Notes:
    #     This has things like helm_version which is not listed in roles/kubernetes-apps/helm/defaults/main.yml
    #   URL:
    #     https://github.com/kubernetes-sigs/kubespray/blob/master/roles/download/defaults/main.yml
    # 
    # Helm
    #   https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes-apps/helm/defaults/main.yml
    helm_enabled: true
    #
    # kubespray_ingress_nginx_enabled
    # dns_cores_per_replica
    # dns_nodes_per_replica
    # populate_inventory_to_hosts_file=false
    # dns_mode
    # cluster_name
    # kube_network_plugin
    # local_volume_provisioner_enabled
