---
vagrant:
  boxes:
    - name: test-1
    - name: test-2
    - name: test-3
  global:
    # Only create the first 'limit_num_boxes' boxes
    #limit_boxes: "yes"
    #limit_num_boxes: 1
    memory: 4096
    cpus: 4
    boot_timeout: 300
    # This ends up slightly different depending on the vagrant provider right now.
    # For Virtualbox: /dev/nvme0n1, /dev/nvme0n2, etc.
    # For libvirt:    /dev/nvme0n1, /dev/nvme1n1, etc.
    # This is due to how Virtualbox only supports one nvme storage controller
    nvme_disks:
      data:
        size: 102401
      data2:
        size: 102401
    #
    # You can use force_provider if your OS defaults differ from the default
    # heuristics on Vagrantfile.
    #force_provider: "libvirt"
    #force_provider: "virtualbox"
    #
    virtualbox_cfg:
      enabled: "true" # If not "true" then omit from Vagrantfile
      # https://app.vagrantup.com/ubuntu/boxes/bionic64
      box: "generic/ubuntu1804"
      enable_sse4: "true"
      # can be vdi, vmdk, vhd
      nvme_disk_postfix: 'vdi'
      # On the current directory
      nvme_disk_path: '.vagrant/nvme_disks/' # Default to a path relative to the Vagrantfile (common for VirtualBox)
      #nvme_disk_path: '/some-absolute-path/' # May be useful
      #nvme_disk_path: '$HOME/.local/share/libvirt/images/' # Some other path in the user's home directory
      nvme_disk_prefix: 'nvme_disk'
      # To stress test a virtual nvme controller you could peg all disks onto
      # one controller. We want to avoid this as our focus is testing filesystems
      # and not storage controllers however Virtualbox currently only supports
      # one nvme storage controller. Set this to true only if you are adding
      # support for this upstream to Virtualbox.
      nvme_controller_per_disk: false
      # If true, then append to 'nvme_disk_path' the name of the project.
      # The project name is taken from the parent directory of the directory holding the Vagrantfiile.
      # So, if the Vagrantfile is in /a/b/c/PROJECT_NAME/foo/Vagrantfile then 'PROJECT_NAME' is used
      append_project_name_to_nvme_disk_path: 'true'
    libvirt_cfg:
      enabled: "true" # If not "true" then omit from Vagrantfile
      # Setup with: vagrant box add <BOX> --provider=libvirt
      box: "generic/ubuntu1804"
      machine_type: 'q35'
      nvme_disk_postfix: 'qcow2'
      nvme_disk_id_prefix: 'drv'
      # Fedora uses qemu here by default. Override with the environment variable
      # KDEVOPS_VAGRANT_QEMU_GROUP. If not sure edit /etc/libvirt/qemu.conf user
      # and group settings. If using apparmor / selinux you may run into snags,
      # but that is out of scope of this project.
      qemu_group: 'qemu' # If this is uncommented, set the group on the storage files to this
      # In case you use a development version of qemu
      # emulator_path: '/usr/local/bin/qemu-system-x86_64'
      emulator_path: '/usr/bin/qemu-system-x86_64'
      # These are all valid
      nvme_disk_path: '$HOME/.local/share/libvirt/images/' # Default to something that works for QEMU://session
      #nvme_disk_path: '/var/lib/libvirt/images/' # Absolute path (for QEMU://sytem)
      #nvme_disk_path: '.vagrant/nvme_disks/' # Relative to the Vagrantfile (requires setting perms)
      #
      # If true, then append to 'nvme_disk_path' the name of the project.
      # The project name is taken from the parent directory of the directory holding the Vagrantfiile.
      # So, if the Vagrantfile is in /a/b/c/PROJECT_NAME/foo/Vagrantfile then 'PROJECT_NAME' is used
      append_project_name_to_nvme_disk_path: 'true'

ansible:
  # build/requirements.yaml
  roles:
    - src: "https://github.com/pwyoung/ansible-role-sanity-check"
      name: "ansible-role-sanity-check"
      scm: "git"
      version: "0.0.2"
  # build/playbooks/<playbooks> and build/run-playbooks.sh
  playbooks:
    - name: "ansible-playbook-sanity-check"
      src: "https://github.com/pwyoung/ansible-playbook-sanity-check"
      method: "git"
      version: "0.0.1"
      entry: "playbook.yaml" # Defaults to site.yml
    - name: "ansible-playbook-deploy-kubespray"
      scm: "git"
      src: "https://github.com/pwyoung/ansible-playbook-deploy-kubespray.git"
      version: "dev" 
  # build/inventory.yml
  inventory:
    all:
      vars:
        ansible_python_interpreter: "/usr/bin/python3"
      hosts:
        test-1:
        test-2:
        test-3:
      children:
        kube-master:
          hosts:
            test-1:
            test-2:
        kube-test:
          hosts:
            test-1:
            test-2:
            test-3:
        etcd:
          hosts:
            test-1:
            test-2:
            test-3:
        k8s-cluster:
          children:
            kube-master:
            kube-test:
        calico-rr:
          hosts: {}
  # build/extra_vars.yml
  # NB: DON'T PUT SECRETS HERE
  # Special Embedded use here: 
  #   The playbook above, 'ansible-playbook-deploy-kubespray', installs Ansible onto the first master node
  #   and runs Kubespray there.
  #   That's done for portability, reproducability, and isolation (security) reasons.
  #   The local file (build/extra_vars.yaml) is copied to the control node and used to modulate the run there.
  #   So, make sure to not include any "real" secrets in this file (that can't be on the control node).
  extra_vars:
    # Kubespray generic/download parameters:
    #   Notes:
    #     This has things like helm_version which is not listed in roles/kubernetes-apps/helm/defaults/main.yml
    #   URL:
    #     https://github.com/kubernetes-sigs/kubespray/blob/master/roles/download/defaults/main.yml
    # 
    # Helm
    #   https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes-apps/helm/defaults/main.yml
    helm_enabled: true
    #
    # Local Volumes
    #   https://github.com/kubernetes-sigs/kubespray/tree/master/roles/kubernetes-apps/external_provisioner/local_volume_provisioner
    local_volume_provisioner_enabled: true    
    #
    # kubespray_ingress_nginx_enabled
    # dns_cores_per_replica
    # dns_nodes_per_replica
    # populate_inventory_to_hosts_file=false
    # dns_mode
    # cluster_name
    # kube_network_plugin

